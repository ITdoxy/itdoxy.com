---
title: "Как установить n8n локально с AI Starter Kit (Docker Compose)?"
slug: как-установить-n8n-локально-с-ai-starter-kit-docker-compose
date: 2025-09-23T00:00:00+03:00
author: Эрик Власкин
authorUrl: https://t.me/DramblukerBlog
categories:
  - Искусственный интеллект
draft: false
---

![](/images/uploads/как-установить-n8n-локально-с-ai-starter-kit-docker-compose.jpg)

## Введение: Что такое n8n и зачем вам нужен AI Starter Kit

n8n — это гибкая платформа для автоматизации рабочих процессов с использованием искусственного интеллекта, которая стала настоящим "святым Граалем" для технических команд. Пользователи описывают n8n как "зверя для автоматизации", где "возможно всё". Эта платформа позволяет соединять различные приложения с API и управлять данными между ними с минимальным использованием кода или вообще без него.

Особенно ценной особенностью n8n является её глубокая интеграция с технологиями искусственного интеллекта, что позволяет создавать продвинутых AI-агентов. Однако многие пользователи не знают, что n8n можно не только использовать в облаке, но и развернуть полностью локально, сохраняя полный контроль над своими данными и AI-моделями.

Именно здесь на помощь приходит **AI Starter Kit** — готовый шаблон на основе Docker Compose, который упрощает развертывание полноценной локальной среды для создания AI-автоматизаций. В этой статье мы подробно разберем процесс установки и настройки n8n с AI Starter Kit.

## Почему стоит выбрать локальную установку n8n с AI Starter Kit

Прежде чем перейти к установке, давайте разберемся, какие преимущества дает вам локальная установка n8n с AI Starter Kit:

- **Полный контроль и безопасность данных**: Все ваши данные остаются на вашем сервере, что критически важно для работы с конфиденциальной информацией
- **Локальное выполнение AI-моделей**: Возможность запускать LLM (Large Language Models) без отправки данных в облако
- **Работа с конфиденциальными данными**: Анализ финансовых отчетов, внутренних PDF и других корпоративных документов без риска утечки
- **Гибкость и расширяемость**: Используйте визуальный конструктор и добавляйте код по мере необходимости
- **Более 500 интеграций**: Подключите n8n к вашим существующим инструментам и сервисам

AI Starter Kit упрощает этот процесс, предоставляя готовую инфраструктуру, включающую все необходимые компоненты для работы с AI прямо "из коробки".

## Что вы получите после установки?

После успешной установки с помощью AI Starter Kit у вас будет полностью локальная среда, включающая:

- **✅ Self-hosted n8n**: Платформа для автоматизации с более чем 400 интеграциями и продвинутыми AI-компонентами
- **✅ Ollama**: Сервис для запуска локальных больших языковых моделей (LLM)
- **✅ Qdrant**: Высокопроизводительная векторная база данных
- **✅ PostgreSQL**: Надежная база данных для хранения данных

Это позволит вам безопасно создавать AI-агентов, анализировать конфиденциальные документы и разрабатывать умных чат-ботов, не опасаясь утечки данных.

## Предварительные требования

Прежде чем приступить к установке, убедитесь, что на вашем компьютере установлены следующие компоненты:

- **Git**: Для клонирования репозитория с GitHub
- **Docker и Docker Compose**: AI Starter Kit использует Docker для развертывания. Docker Desktop для Mac, Windows и Linux является отличным вариантом, так как включает в себя и Docker Engine, и Docker Compose

**Важное замечание**: Самостоятельный хостинг n8n рекомендуется для опытных пользователей. Вы должны обладать базовыми техническими знаниями в области работы с терминалом, контейнерами и управления конфигурационными файлами. Ошибки в настройке могут привести к проблемам с безопасностью.

## Пошаговая установка n8n с AI Starter Kit

### Шаг 1: Клонирование репозитория

Откройте терминал и выполните следующую команду для клонирования репозитория:

```bash
git clone https://github.com/n8n-io/self-hosted-ai-starter-kit.git
```

Эта команда скачает все необходимые файлы на ваш компьютер.

### Шаг 2: Переход в директорию проекта

```bash
cd self-hosted-ai-starter-kit
```

### Шаг 3: Настройка файла конфигурации

Создайте файл конфигурации .env на основе примера:

```bash
cp .env.example .env
```

**Обязательно откройте файл .env в текстовом редакторе и измените секреты и пароли по умолчанию для обеспечения безопасности вашего экземпляра.** Это критически важный шаг, который нельзя пропускать.

### Шаг 4: Запуск с помощью Docker Compose

Финальный шаг — запуск всего стека. Команда для запуска зависит от вашего оборудования:

#### Для пользователей с GPU от Nvidia

```bash
docker compose --profile gpu-nvidia up
```

*Примечание: Если вы ранее не использовали GPU Nvidia с Docker, возможно, потребуется дополнительная настройка в соответствии с инструкциями Ollama Docker.*

#### Для пользователей с GPU от AMD (только на Linux)

```bash
docker compose --profile gpu-amd up
```

#### Для всех остальных (включая Mac / Apple Silicon и системы без GPU)

```bash
docker compose --profile cpu up
```

#### Особый случай: пользователи Mac с Apple Silicon
Если вы используете Mac с процессором M1 или новее, вы не можете напрямую предоставить GPU Docker-контейнеру. Для более высокой производительности:

1. Установите Ollama на macOS согласно [официальной инструкции](https://ollama.com/)
2. В файле `.env` измените значение переменной `OLLAMA_HOST` на `host.docker.internal:11434`
3. Запустите Docker Compose без указания профиля:
   ```bash
   docker compose up
   ```
4. После запуска, в интерфейсе n8n зайдите в Credentials, найдите "Local Ollama service" и измените Base URL на `http://host.docker.internal:11434/`

После выполнения команды Docker Compose начнет скачивать необходимые образы и запускать контейнеры. При первом запуске это может занять некоторое время (10-20 минут), так как Ollama будет загружать большую языковую модель (например, Llama3.2). Вы можете отслеживать прогресс в логах Docker.

## Первые шаги после установки

Когда все контейнеры успешно запустятся (вы увидите сообщение "Server running on port 5678" в логах), проделайте следующие шаги:

1. **Откройте в браузере** http://localhost:5678/
2. **Создайте учетную запись администратора** (этот шаг выполняется только один раз)
3. **Откройте пример рабочего процесса (опционально)**: Перейдите по ссылке http://localhost:5678/workflow/srOnR8PAY3u4RSwb, чтобы открыть демонстрационный AI-чат
4. **Запустите чат (опционально)**: Нажмите кнопку Chat в нижней части экрана, чтобы начать взаимодействовать с вашим локальным AI-агентом

## Что можно делать с вашей новой локальной AI-средой

Теперь, когда у вас есть полностью локальная установка n8n с AI Starter Kit, вы можете:

- Создавать собственные рабочие процессы с использованием узлов Ollama для языковых моделей
- Использовать Qdrant для векторного хранения и поиска
- Разрабатывать AI-агентов для обработки внутренних документов
- Интегрировать n8n с вашими корпоративными системами через более чем 500 доступных интеграций
- Создавать многошаговые агентские системы на одном экране

## Заключение и рекомендации

Поздравляем! Вы успешно развернули локальную среду для AI-автоматизации с помощью n8n и AI Starter Kit. Теперь вы можете создавать сложные рабочие процессы с использованием искусственного интеллекта, сохраняя полный контроль над своими данными.

**Важно помнить**: Данный стартовый набор предназначен для быстрого старта, прототипирования и проверки концепций (proof-of-concept). Он не оптимизирован для производственной (production) среды и требует дополнительной настройки и усиления безопасности перед использованием в реальных бизнес-процессах.

Для производственной (production) среды рекомендуется:
- Настроить HTTPS с помощью обратного прокси (Nginx, Traefik)
- Настроить регулярные резервные копии базы данных
- Настроить систему мониторинга
- Добавить дополнительные меры безопасности

Если вы только начинаете знакомство с n8n, обратите внимание на [доступные шаблоны рабочих процессов](https://n8n.io/workflows/), которые помогут вам быстрее освоить возможности платформы и создать свои первые автоматизации.

С локальной установкой n8n вы получаете мощный инструмент для создания AI-решений, который можно адаптировать под любые ваши задачи, сохраняя при этом полный контроль над данными и инфраструктурой.
